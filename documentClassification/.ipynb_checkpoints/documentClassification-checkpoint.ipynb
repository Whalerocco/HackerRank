{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f95d3c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task\n",
    "# https://www.hackerrank.com/challenges/document-classification/problem\n",
    "\n",
    "# Useful links\n",
    "# https://scikit-learn.org/stable/auto_examples/text/plot_document_classification_20newsgroups.html#sphx-glr-auto-examples-text-plot-document-classification-20newsgroups-py\n",
    "# https://www.docsumo.com/blog/document-classification\n",
    "\n",
    "# !/bin/python3\n",
    "\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "37bc94c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "nrFeatures = 100 # If the text is too short it won't have enough features, maybe can set rest to 0?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "74916d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"trainingdata.txt\", \"r\") as file1:\n",
    "    content = file1.readlines()\n",
    "\n",
    "N = content[0] # Training data size\n",
    "del content[0] # Remove from read data   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "5e2b0d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\00350974\\appdata\\local\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\00350974\\appdata\\local\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\00350974\\appdata\\local\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\00350974\\appdata\\local\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\users\\00350974\\appdata\\local\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\00350974\\appdata\\local\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "3edef5a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\00350974\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\00350974\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk # natural language processing\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "b4714b05",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>champion products ch approves stock split cham...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>computer terminal systems cpml completes sale ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cobanco inc cbco year net shr cts vs dlrs net ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>am international inc am nd qtr jan oper shr lo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>brown forman inc bfd th qtr net shr one dlr vs...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Class\n",
       "0  champion products ch approves stock split cham...      1\n",
       "1  computer terminal systems cpml completes sale ...      2\n",
       "2  cobanco inc cbco year net shr cts vs dlrs net ...      1\n",
       "3  am international inc am nd qtr jan oper shr lo...      1\n",
       "4  brown forman inc bfd th qtr net shr one dlr vs...      1"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Move the data into X (feature matrix) and y (class array)\n",
    "\n",
    "from nltk.tokenize import sent_tokenize #word_tokenize or sent_tokenize (best for longer texts)\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer() # Reduces the words to their base form\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer() # Reduces inflection from words - similar to stemming\n",
    "y = []\n",
    "X = []\n",
    "for line in content:\n",
    "    y.append(int(line[0]))\n",
    "    processed_line = sent_tokenize(line[2:]) # Tokenize the words. Other preprocesses include stop word removal, POS tagging and chunking\n",
    "#     processed_line = [stemmer.stem(i) for i in processed_line] # Takes long time, use if needed\n",
    "    processed_line = [lemmatizer.lemmatize(i) for i in processed_line] \n",
    "    X.append(processed_line[0])\n",
    "\n",
    "df = pd.DataFrame(zip(X, y),\n",
    "           columns =['Text', 'Class'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "ab53c2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visualize the words with a word cloud\n",
    "# # !pip install WordCloud\n",
    "# import wordcloud\n",
    "# wordcloud_pos = wordcloud(width=200,\n",
    "#                          height=500,\n",
    "#                          max_font_size=150).generate(print(df[df['Class'] == 1]))\n",
    "\n",
    "# plt.figure(figsize=(15, 10))\n",
    "# plt.imshow(interpolation = \"bilinear\")\n",
    "# plt.axis(\"off\")\n",
    "# plt.title(f\"Most common words associated with non-toxic comment\", size=20)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "677968c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, Normalizer \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # term frequency\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "8caf629b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2174299837.py, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[183], line 5\u001b[1;36m\u001b[0m\n\u001b[1;33m    - BOW -\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df[\"Text\"], df[\"Class\"], test_size=0.10)\n",
    "\n",
    "# https://realpython.com/python-keras-text-classification/\n",
    "\n",
    "# - Bag Of Words -\n",
    "# Create features from the data\n",
    "tfid = TfidfVectorizer(lowercase=False, max_features=nrFeatures, stop_words =\"english\")  # term frequency\n",
    "X_train_tfidf = tfid.fit_transform(X_train).toarray()\n",
    "X_test_tfidf = tfid.transform(X_test).toarray()\n",
    "\n",
    "# Normalize the vectors - probably not needed\n",
    "norm_TFIDF = Normalizer(copy=False)\n",
    "X_norm_train_tfidf = norm_TFIDF.fit_transform(X_train_tfidf)\n",
    "X_norm_test_tfidf = norm_TFIDF.transform(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "d2c944bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4936, 100)\n"
     ]
    }
   ],
   "source": [
    "print(X_norm_train_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "db5ae448",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4196    1\n",
       "3352    1\n",
       "253     2\n",
       "2279    2\n",
       "4714    8\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "2d947c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, precision_recall_curve\n",
    "\n",
    "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_model.fit(X_norm_train_tfidf, y_train)\n",
    "y_pred = knn_model.predict(X_norm_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "3f16ea23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.96      0.96      0.96       285\n",
      "           2       0.90      0.91      0.90       161\n",
      "           3       0.79      0.90      0.84        30\n",
      "           4       0.60      0.25      0.35        12\n",
      "           5       0.00      0.00      0.00         4\n",
      "           6       0.95      0.95      0.95        21\n",
      "           7       0.74      0.93      0.82        15\n",
      "           8       0.70      0.67      0.68        21\n",
      "\n",
      "    accuracy                           0.91       549\n",
      "   macro avg       0.71      0.70      0.69       549\n",
      "weighted avg       0.90      0.91      0.90       549\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "e0fb6c72",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      0.89      0.94       285\n",
      "           2       0.86      0.63      0.73       161\n",
      "           3       0.67      0.40      0.50        30\n",
      "           4       0.11      0.67      0.19        12\n",
      "           5       0.05      0.25      0.08         4\n",
      "           6       0.50      0.52      0.51        21\n",
      "           7       0.48      0.80      0.60        15\n",
      "           8       0.56      0.43      0.49        21\n",
      "\n",
      "    accuracy                           0.75       549\n",
      "   macro avg       0.53      0.57      0.51       549\n",
      "weighted avg       0.86      0.75      0.79       549\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "nb_model = GaussianNB()\n",
    "nb_model.fit(X_norm_train_tfidf, y_train)\n",
    "y_pred = nb_model.predict(X_norm_test_tfidf)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "4353fba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.98      0.96      0.97       285\n",
      "           2       0.90      0.96      0.92       161\n",
      "           3       0.90      0.93      0.92        30\n",
      "           4       0.83      0.42      0.56        12\n",
      "           5       0.50      0.25      0.33         4\n",
      "           6       0.87      0.95      0.91        21\n",
      "           7       0.76      0.87      0.81        15\n",
      "           8       0.84      0.76      0.80        21\n",
      "\n",
      "    accuracy                           0.93       549\n",
      "   macro avg       0.82      0.76      0.78       549\n",
      "weighted avg       0.93      0.93      0.93       549\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC # Support vector classifier\n",
    "svm_model = SVC()\n",
    "svm_model.fit(X_norm_train_tfidf, y_train)\n",
    "y_pred = svm_model.predict(X_norm_test_tfidf)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "a773710b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_model = svm_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbc9cda",
   "metadata": {},
   "source": [
    "# Analyse training - only works for linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "05725fdb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "coef_ is only available when using a linear kernel",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[181], line 46\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28mprint\u001b[39m(top)\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ax\n\u001b[1;32m---> 46\u001b[0m _ \u001b[38;5;241m=\u001b[39m plot_feature_effects(X_norm_train_tfidf, tfid, ml_model)\u001b[38;5;241m.\u001b[39mset_title(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage feature effect on the original data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[181], line 3\u001b[0m, in \u001b[0;36mplot_feature_effects\u001b[1;34m(X_norm_train_tfidf, tfid, ml_model)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot_feature_effects\u001b[39m(X_norm_train_tfidf, tfid, ml_model):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# learned coefficients weighted by frequency of appearance\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     average_feature_effects \u001b[38;5;241m=\u001b[39m ml_model\u001b[38;5;241m.\u001b[39mcoef_ \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(X_norm_train_tfidf\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\u001b[38;5;241m.\u001b[39mravel()\n\u001b[0;32m      4\u001b[0m     feature_names \u001b[38;5;241m=\u001b[39m tfid\u001b[38;5;241m.\u001b[39mget_feature_names_out()\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(X_norm_train_tfidf\u001b[38;5;241m.\u001b[39mtarget_names):\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:656\u001b[0m, in \u001b[0;36mBaseLibSVM.coef_\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    649\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Weights assigned to the features when `kernel=\"linear\"`.\u001b[39;00m\n\u001b[0;32m    650\u001b[0m \n\u001b[0;32m    651\u001b[0m \u001b[38;5;124;03mReturns\u001b[39;00m\n\u001b[0;32m    652\u001b[0m \u001b[38;5;124;03m-------\u001b[39;00m\n\u001b[0;32m    653\u001b[0m \u001b[38;5;124;03mndarray of shape (n_features, n_classes)\u001b[39;00m\n\u001b[0;32m    654\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    655\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 656\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoef_ is only available when using a linear kernel\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    658\u001b[0m coef \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_coef()\n\u001b[0;32m    660\u001b[0m \u001b[38;5;66;03m# coef_ being a read-only property, it's better to mark the value as\u001b[39;00m\n\u001b[0;32m    661\u001b[0m \u001b[38;5;66;03m# immutable to avoid hiding potential bugs for the unsuspecting user.\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: coef_ is only available when using a linear kernel"
     ]
    }
   ],
   "source": [
    "def plot_feature_effects(X_norm_train_tfidf, tfid, ml_model):\n",
    "    # learned coefficients weighted by frequency of appearance\n",
    "    average_feature_effects = ml_model.coef_ * np.asarray(X_norm_train_tfidf.mean(axis=0)).ravel()\n",
    "    feature_names = tfid.get_feature_names_out()\n",
    "    \n",
    "    for i, label in enumerate(X_norm_train_tfidf.target_names):\n",
    "        top5 = np.argsort(average_feature_effects[i])[-5:][::-1]\n",
    "        if i == 0:\n",
    "            top = pd.DataFrame(feature_names[top5], columns=[label])\n",
    "            top_indices = top5\n",
    "        else:\n",
    "            top[label] = feature_names[top5]\n",
    "            top_indices = np.concatenate((top_indices, top5), axis=None)\n",
    "    top_indices = np.unique(top_indices)\n",
    "    predictive_words = feature_names[top_indices]\n",
    "\n",
    "    # plot feature effects\n",
    "    bar_size = 0.25\n",
    "    padding = 0.75\n",
    "    y_locs = np.arange(len(top_indices)) * (4 * bar_size + padding)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    for i, label in enumerate(target_names):\n",
    "        ax.barh(\n",
    "            y_locs + (i - 2) * bar_size,\n",
    "            average_feature_effects[i, top_indices],\n",
    "            height=bar_size,\n",
    "            label=label,\n",
    "        )\n",
    "    ax.set(\n",
    "        yticks=y_locs,\n",
    "        yticklabels=predictive_words,\n",
    "        ylim=[\n",
    "            0 - 4 * bar_size,\n",
    "            len(top_indices) * (4 * bar_size + padding) - 4 * bar_size,\n",
    "        ],\n",
    "    )\n",
    "    ax.legend(loc=\"lower right\")\n",
    "\n",
    "    print(\"top 5 keywords per class:\")\n",
    "    print(top)\n",
    "\n",
    "    return ax\n",
    "\n",
    "\n",
    "_ = plot_feature_effects(X_norm_train_tfidf, tfid, ml_model).set_title(\"Average feature effect on the original data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a36f996",
   "metadata": {},
   "source": [
    "# CLASSIFY AN INPUT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "1418ed0e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brooklyn union gas co bu sets payout qtrly div cts vs cts prior pay may one record april reuter \n",
      "TfidfVectorizer(lowercase=False, max_features=100, stop_words='english')\n",
      "<enumerate object at 0x000002E48D98FE20>\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[192], line 41\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[38;5;66;03m#ADD FUNCTION TO PROCESS DATA, STR->VECTORIZED\u001b[39;00m\n\u001b[0;32m     39\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClass is: \u001b[39m\u001b[38;5;124m\"\u001b[39m, ml_model\u001b[38;5;241m.\u001b[39mpredict(norm_input_tfidf))\n\u001b[1;32m---> 41\u001b[0m main()\n",
      "Cell \u001b[1;32mIn[192], line 30\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28menumerate\u001b[39m(features[\u001b[38;5;241m0\u001b[39m]))        \n\u001b[0;32m     29\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(features[\u001b[38;5;241m0\u001b[39m]):\n\u001b[1;32m---> 30\u001b[0m             input_vectors_tfidf[\u001b[38;5;241m1\u001b[39m, i] \u001b[38;5;241m=\u001b[39m value\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m#         input_vectors_tfidf = tfid.fit_transform(X).toarray() # This variable has too few features\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m#         print(tfid.get_feature_names_out())\u001b[39;00m\n\u001b[0;32m     34\u001b[0m         \n\u001b[0;32m     35\u001b[0m         \u001b[38;5;66;03m# Normalize the vectors\u001b[39;00m\n\u001b[0;32m     36\u001b[0m         norm_TFIDF \u001b[38;5;241m=\u001b[39m Normalizer(copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mIndexError\u001b[0m: index 1 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    warnings.filterwarnings('ignore')\n",
    "    # Read input from stdio\n",
    "#     input = sys.stdin.readline\n",
    "    nrClassifications = 1 #int(input()) # first line says how many more lines there are\n",
    "\n",
    "    for i in range(nrClassifications):\n",
    "        line = input()\n",
    "        X = []\n",
    "        stemmer = PorterStemmer() # Reduces the words to their base form\n",
    "        lemmatizer = WordNetLemmatizer() # Reduces inflection from words - similar to stemming\n",
    "\n",
    "        processed_line = sent_tokenize(line) # Tokenize the words. Other preprocesses include stop word removal, POS tagging and chunking\n",
    "    #     processed_line = [stemmer.stem(i) for i in processed_line] # Takes long time, use if needed\n",
    "        processed_line = [lemmatizer.lemmatize(i) for i in processed_line] \n",
    "        X.append(processed_line[0])\n",
    "        \n",
    "        # VS CODE STILL HAS ORIGINAL CODE, HOW TO DEAL WITH MISSING FEATURES?\n",
    "        # Create features from the data\n",
    "        tfid = TfidfVectorizer(lowercase=False, max_features=nrFeatures, stop_words =\"english\")  # term frequency, can remove stop words here\n",
    "        print(tfid)\n",
    "        \n",
    "        # Create a vector of zeros with the same number of features as the model's training data\n",
    "        zero_vector = np.zeros([1, nrFeatures])\n",
    "\n",
    "        # Get feature values from input and update the corresponding positions in the zero vector\n",
    "        input_vectors_tfidf = zero_vector.copy()\n",
    "        features = tfid.fit_transform(X).toarray()\n",
    "        print(enumerate(features[0]))        \n",
    "        for i, value in enumerate(features[0]):\n",
    "            input_vectors_tfidf[1, i] = value\n",
    "        \n",
    "#         input_vectors_tfidf = tfid.fit_transform(X).toarray() # This variable has too few features\n",
    "#         print(tfid.get_feature_names_out())\n",
    "        \n",
    "        # Normalize the vectors\n",
    "        norm_TFIDF = Normalizer(copy=False)\n",
    "        norm_input_tfidf = norm_TFIDF.fit_transform(input_vectors_tfidf)\n",
    "        #ADD FUNCTION TO PROCESS DATA, STR->VECTORIZED\n",
    "        print(\"Class is: \", ml_model.predict(norm_input_tfidf))\n",
    "        \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "2e245ccb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[190], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(features[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'features' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e979dae7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
